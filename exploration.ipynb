{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Flowers102'\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                        [0.229, 0.224, 0.225])])\n",
    "\n",
    "testval_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.Flowers102(root='./data', split='train', download=True, transform=train_transform)\n",
    "val_dataset = torchvision.datasets.Flowers102(root='./data', split='val', download=True, transform=testval_transform)\n",
    "test_dataset = torchvision.datasets.Flowers102(root='./data', split='test', download=True, transform=testval_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(pil_image):    \n",
    "    # -------- Resize with Aspect Ratio maintained--------- #\n",
    "    # First fixing the short axes\n",
    "    if pil_image.size[0] > pil_image.size[1]:\n",
    "        pil_image.thumbnail((10000000, 256))\n",
    "    else:\n",
    "        pil_image.thumbnail((256, 100000000))\n",
    "    \n",
    "    # ---------Crop----------- #\n",
    "    left_margin = (pil_image.width - 224) / 2\n",
    "    bottom_margin = (pil_image.height - 224) / 2\n",
    "    right_margin = left_margin + 224\n",
    "    top_margin = bottom_margin + 224\n",
    "    \n",
    "    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
    "    \n",
    "    # --------- Convert to np then Normalize ----------- #\n",
    "    np_image = np.array(pil_image) / 255\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    np_image = (np_image -mean) / std\n",
    "    \n",
    "    # --------- Transpose to fit PyTorch Axes ----------#\n",
    "    np_image = np_image.transpose([2, 0, 1])\n",
    "    \n",
    "    return np_image\n",
    "\n",
    "def imshow(pt_image, ax = None, title = None):\n",
    "    '''\n",
    "    Takes in a PyTorch-compatible image with [Ch, H, W],\n",
    "    Convert it back to [H, W, Ch], \n",
    "    Undo the preprocessing,\n",
    "    then display it on a grid\n",
    "    '''\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # --------- Transpose ----------- #\n",
    "    plt_image = pt_image.transpose((1, 2, 0))\n",
    "    \n",
    "    # --------- Undo the preprocessing --------- #\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    plt_image = plt_image * std + mean\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "        \n",
    "    # Image need to be clipped between 0 and 1 or it looks noisy\n",
    "    plt_image = np.clip(plt_image, 0, 1)\n",
    "    \n",
    "    # this imshow is a function defined in the plt module\n",
    "    ax.imshow(plt_image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77, 77, 77, ..., 62, 62, 62]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_path = './data/flowers-102/imagelabels.mat'\n",
    "label_arr = scp.loadmat(label_path)['labels']\n",
    "label_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1020)\n",
      "(1, 1020)\n",
      "(1, 6149)\n"
     ]
    }
   ],
   "source": [
    "split_path = './data/flowers-102/setid.mat'\n",
    "data_splits = scp.loadmat(split_path)\n",
    "train_split = data_splits['trnid']\n",
    "print(train_split.shape)\n",
    "val_split = data_splits['valid']\n",
    "print(val_split.shape)\n",
    "test_split = data_splits['tstid']\n",
    "print(test_split.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Select a random sample\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "# image = image_preprocessing(images)\n",
    "# imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_sample_counts = {}\n",
    "\n",
    "# for c in range(1, 103):\n",
    "#     class_sample_counts[c] = train_dataset._labels.count(c)\n",
    "\n",
    "# # Plot showing the class imbalance\n",
    "# plt.figure(figsize=(22, 5))    \n",
    "# plt.bar(range(len(class_sample_counts)), list(class_sample_counts.values()), align='center')\n",
    "# plt.xticks(range(len(class_sample_counts)), list(class_sample_counts.keys()))\n",
    "# plt.xticks(rotation=80)\n",
    "# plt.title('Samples per Class')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
